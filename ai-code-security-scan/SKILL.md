---
name: ai-code-security-scan
description: Scans AI-generated code and AI/ML codebases for security vulnerabilities based on OWASP LLM Top 10:2025. Detects prompt injection risks, sensitive data exposure, supply chain attacks (slopsquatting), excessive agency patterns, and AI-specific code weaknesses. Use when reviewing AI-generated code, auditing LangChain/LlamaIndex projects, or securing AI agent systems.
---

# AI Code Security Scan

Security scanner for AI-generated code and AI/ML applications, aligned with [OWASP LLM Top 10:2025](https://genai.owasp.org/llm-top-10/).

## When to Use

- Reviewing code generated by AI assistants (Cursor, Copilot, Claude)
- Auditing LangChain, LlamaIndex, or similar AI framework projects
- Securing multi-agent systems and AI workflows
- Pre-deployment security checks for AI applications
- Detecting hallucinated package dependencies

## Scan Types

### Quick Scan
Fast scan for critical AI-specific vulnerabilities:
```
Task Progress:
- [ ] Check for prompt injection patterns
- [ ] Detect hardcoded API keys (OpenAI, Anthropic, etc.)
- [ ] Identify excessive agency risks
- [ ] Scan for hallucinated dependencies
```

### Full Scan
Comprehensive OWASP LLM Top 10:2025 assessment:
```
Task Progress:
- [ ] LLM01: Prompt Injection patterns
- [ ] LLM02: Sensitive Information Disclosure
- [ ] LLM03: Supply Chain vulnerabilities
- [ ] LLM04: Data and Model Poisoning risks
- [ ] LLM05: Insecure Output Handling
- [ ] LLM06: Excessive Agency
- [ ] LLM07: System Prompt Leakage
- [ ] LLM08: Vector/Embedding Weaknesses
- [ ] LLM09: Misinformation risks
- [ ] LLM10: Unbounded Consumption
```

## Scan Workflow

### Step 1: Project Discovery

Identify AI/ML framework usage:

```bash
# Detect AI frameworks
grep -rE "from (langchain|llama_index|openai|anthropic|transformers)" --include="*.py" .
grep -rE "import (OpenAI|Anthropic|ChatOpenAI)" --include="*.py" .
```

Framework detection patterns:
- **LangChain**: `from langchain`, `ChatOpenAI`, `AgentExecutor`
- **LlamaIndex**: `from llama_index`, `VectorStoreIndex`
- **OpenAI SDK**: `from openai`, `OpenAI()`
- **Anthropic SDK**: `from anthropic`, `Anthropic()`
- **Transformers**: `from transformers`, `AutoModel`

### Step 2: Run Automated Scans

Execute utility scripts:

```bash
# Scan for AI-specific patterns
python scripts/ai-code-analyzer.py /path/to/project

# Detect hallucinated packages
python scripts/hallucination-detector.py /path/to/project
```

### Step 3: Manual Pattern Review

For each vulnerability category, check patterns in:
- [references/owasp-llm-top-10.md](references/owasp-llm-top-10.md) - Full OWASP reference
- [references/ai-code-patterns.md](references/ai-code-patterns.md) - Detection patterns
- [references/prompt-injection.md](references/prompt-injection.md) - Prompt injection guide
- [references/supply-chain.md](references/supply-chain.md) - Supply chain risks

### Step 4: Generate Report

Use template: [assets/report-template.md](assets/report-template.md)

## Critical Detection Patterns

### Prompt Injection (LLM01)

```python
# CRITICAL: User input directly in prompts
prompt = f"Answer this: {user_input}"  # Vulnerable!
chain.invoke({"input": user_input})     # Check input validation

# CRITICAL: RAG injection vectors
docs = retriever.get_relevant_documents(user_query)  # May contain malicious content
```

### Sensitive Data Exposure (LLM02)

```python
# CRITICAL: Logging prompts/responses
logger.info(f"User prompt: {prompt}")      # May log PII
logger.debug(f"LLM response: {response}")  # May log secrets

# CRITICAL: Exposing system prompts
return {"system_prompt": system_prompt}    # Never expose!
```

### Supply Chain (LLM03)

```python
# CRITICAL: Unverified model loading
model = AutoModel.from_pretrained(user_provided_model)  # RCE risk!

# CRITICAL: Dynamic package installation
subprocess.run(["pip", "install", package_name])  # Slopsquatting!
```

### Excessive Agency (LLM06)

```python
# CRITICAL: Unrestricted tool access
tools = load_tools(["shell", "python_repl", "file_management"])  # Too powerful!

# CRITICAL: No human-in-the-loop
agent.run(user_request)  # Autonomous execution without approval
```

## AI Framework-Specific Checks

### LangChain

| Pattern | Risk | Location |
|---------|------|----------|
| `AgentExecutor` without `max_iterations` | Unbounded consumption | Agent configs |
| `Tool` with `shell` capability | RCE via prompt injection | Tool definitions |
| `ConversationBufferMemory` with PII | Data exposure | Memory configs |
| `from_pretrained` without verification | Supply chain | Model loading |

### LlamaIndex

| Pattern | Risk | Location |
|---------|------|----------|
| `VectorStoreIndex` without access control | Data leakage | Index creation |
| `ServiceContext` with verbose logging | Sensitive data exposure | Context configs |
| External data sources without validation | RAG poisoning | Data connectors |

### OpenAI/Anthropic SDKs

| Pattern | Risk | Location |
|---------|------|----------|
| API key in code | Credential exposure | Client initialization |
| `function_call` without validation | Prompt injection | Tool use |
| No rate limiting | Unbounded consumption | API calls |

## Severity Classification

| Severity | Description | Action |
|----------|-------------|--------|
| CRITICAL | Exploitable with severe impact (RCE, data breach) | Immediate fix |
| HIGH | Significant risk (prompt injection, excessive agency) | Fix before deployment |
| MEDIUM | Potential issue (logging sensitive data) | Fix in next release |
| LOW | Best practice violation | Consider fixing |
| INFO | Suggestion for improvement | Optional |

## Key Files to Scan

### Always Check
- `**/*.py` - Python source files
- `**/agent*.py`, `**/chain*.py` - Agent/chain definitions
- `**/tools*.py` - Tool definitions
- `**/prompt*.py`, `**/template*.py` - Prompt templates
- `requirements.txt`, `pyproject.toml` - Dependencies

### High Priority
- `**/memory*.py` - Memory configurations
- `**/retriever*.py` - RAG retrievers
- `**/callback*.py` - Callback handlers (may log sensitive data)
- `.env*` - Environment files (check for committed secrets)

## Output Format

```
[SEVERITY] LLM0X - Category: Description
  File: path/to/file.py:lineNumber
  Code: <relevant code snippet>
  Risk: <explanation of the security risk>
  Fix: <recommended remediation>
```

## Integration with CI/CD

Add to GitHub Actions:

```yaml
- name: AI Security Scan
  run: |
    python .claude/skills/ai-code-security-scan/scripts/ai-code-analyzer.py ./src
    if [ $? -ne 0 ]; then
      echo "AI security vulnerabilities detected!"
      exit 1
    fi
```

## References

- [references/owasp-llm-top-10.md](references/owasp-llm-top-10.md) - Complete OWASP LLM Top 10:2025
- [references/ai-code-patterns.md](references/ai-code-patterns.md) - Detection regex patterns
- [references/prompt-injection.md](references/prompt-injection.md) - Prompt injection guide
- [references/supply-chain.md](references/supply-chain.md) - Supply chain risks (slopsquatting, model poisoning)
